# -*- coding: utf-8 -*-
"""RSTSE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BvbAyFqQN6w-G0f5bI0gbdIf_lfO3FkT
"""

!pip install memory_profiler

import pandas as pd
import numpy as np
import csv
from geopy.distance import geodesic
from collections import OrderedDict
import ast

checkin_data=pd.read_csv('train_timestamp.csv')
friends_data = pd.read_csv('Gowalla_social_relations.txt', sep='\t', header=None, names = ['uid1', 'uid2'])

train=checkins_data
train.drop(['timestamp'], axis=1, inplace=True)
train.rename(columns={'uid1': 'userid'}, inplace=True)

"""**RSTSE-IMPLICIT**"""

import numpy as np
import pandas as pd

def calculate_kl_divergence(p1, p2, epsilon=1e-10):
    p1 = np.array(p1)
    p2 = np.array(p2)

    p1 = p1 / np.sum(p1)
    p2_sum = np.sum(p2)

    if p2_sum == 0:
        return np.inf

    p2 = p2 / p2_sum

    kl_divergence = np.sum(p1 * np.log((p1 + epsilon) / (p2 + epsilon)))
    return kl_divergence

def calculate_preferences_distribution(user_checkins, all_locations):
    user_distribution = user_checkins.value_counts(normalize=True).sort_index()
    user_distribution = user_distribution.reindex(all_locations, fill_value=0)
    return user_distribution

def calculate_weights_kl_divergence(target_user_preferences, friend_preferences):
    weights_dict = {}
    for friend_id, friend_preferences in friend_preferences.items():
        kl_divergence = calculate_kl_divergence(target_user_preferences, friend_preferences)
        if np.isfinite(kl_divergence):
            weight = 1 / (kl_divergence + 1e-10)
        else:
            weight = 0

        weights_dict[friend_id] = weight

    return weights_dict

all_locations = checkin_data['poi-id'].unique()

all_weights = {}

for target_user_id in checkin_data['uid1'].unique():
    target_user_checkins = checkin_data[checkin_data['uid1'] == target_user_id]['poi-id']
    target_user_friends_uid1 = friends_data[friends_data['uid1'] == target_user_id]['uid2'].tolist()
    target_user_friends_uid2 = friends_data[friends_data['uid2'] == target_user_id]['uid1'].tolist()
    target_user_friends = target_user_friends_uid1 + target_user_friends_uid2

    friends_checkins = {}
    for friend_id in target_user_friends:
        friend_checkins_data = checkin_data[checkin_data['uid1'] == friend_id]
        friends_checkins[friend_id] = friend_checkins_data['poi-id']
    target_user_preferences = calculate_preferences_distribution(target_user_checkins, all_locations)
    friends_preferences = {friend_id: calculate_preferences_distribution(friends_checkins[friend_id], all_locations) for friend_id in target_user_friends}
    weights_kl_divergence = calculate_weights_kl_divergence(target_user_preferences, friends_preferences)
    all_weights[target_user_id] = weights_kl_divergence

for user_id, weights in all_weights.items():
    print(f"User ID: {user_id}, KL Divergence Weights: {weights}")


def calculate_one_dimensional_power_law_weight(ts, beta):
    if ts < 0 or beta <= 1:
        return 0
    else:
        return (beta - 1) * (1 + ts) ** (-beta)

def calculate_recency_weight(target_user_last_checkin_timestamp, friend_last_checkin_timestamp, beta):
    ts_uv = np.abs(target_user_last_checkin_timestamp - friend_last_checkin_timestamp)

    if ts_uv == 0 or beta <= 1:
        return 0
    else:
        recency_weight = 1 - (1 / (1 + ts_uv ** (1 - beta)))
        return recency_weight

def calculate_weights_one_dimensional_power_law(target_user_last_checkin_timestamp, friend_last_checkin_timestamp, beta):
    ts_uv = np.abs(target_user_last_checkin_timestamp - friend_last_checkin_timestamp)
    recency_weight = calculate_recency_weight(target_user_last_checkin_timestamp, friend_last_checkin_timestamp, beta)
    power_law_weight = calculate_one_dimensional_power_law_weight(ts_uv, beta)
    if np.isnan(power_law_weight) or np.isnan(recency_weight):
        return 0
    else:
        return power_law_weight * recency_weight

def calculate_weights_for_all_users(friends_data, checkin_data, beta):
    all_weights = {}

    for target_user_id in friends_data['uid1'].unique():
        target_user_data = checkin_data[checkin_data['uid1'] == target_user_id]
        target_user_last_checkin_timestamp = target_user_data['timestamp'].max()
        friends_user_ids = target_user_data['uid1'].tolist()

        weights = []
        for friend_id in friends_data[friends_data['uid1'] == target_user_id]['uid2']:
            friend_data = checkin_data[checkin_data['uid1'] == friend_id]
            friend_last_checkin_timestamp = friend_data['timestamp'].max()
            weight = calculate_weights_one_dimensional_power_law(target_user_last_checkin_timestamp, friend_last_checkin_timestamp, beta)
            weights.append((friend_id, weight))

        all_weights[target_user_id] = weights

    return all_weights

all_weights2 = {}

for target_user_id in checkin_data['uid1'].unique():
    target_user_checkins = checkin_data[checkin_data['uid1'] == target_user_id]['poi-id']
    target_user_friends_uid1 = friends_data[friends_data['uid1'] == target_user_id]['uid2'].tolist()
    target_user_friends_uid2 = friends_data[friends_data['uid2'] == target_user_id]['uid1'].tolist()
    target_user_friends = target_user_friends_uid1 + target_user_friends_uid2

beta = 1.5

all_weights2 = calculate_weights_for_all_users(friends_data, checkin_data, beta)

for user_id, weights in all_weights2.items():
    print(f"User ID: {user_id}, Weights: {weights}")

combined_weights = {}
alpha = 0.4
for user_id in set(all_weights.keys()).union(all_weights2.keys()):
    weights_kl_divergence = dict(all_weights.get(user_id, []))
    weights_power_law = dict(all_weights2.get(user_id, []))

    friend_ids = set(weights_kl_divergence.keys()).union(weights_power_law.keys())
    combined_weights[user_id] = [(friend_id, alpha* weights_kl_divergence.get(friend_id, 0) + (1-alpha)* weights_power_law.get(friend_id, 0)) for friend_id in friend_ids]

for user_id, weights in combined_weights.items():
    print(f"User ID: {user_id}, Combined Weights: {weights}")

sorted_combined_weights = {}

for user_id, weights in combined_weights.items():
    sorted_weights = sorted(weights, key=lambda x: x[1], reverse=True)
    sorted_combined_weights[user_id] = sorted_weights

for user_id, sorted_weights in sorted_combined_weights.items():
    print(f"User ID: {user_id}, Sorted Combined Weights: {sorted_weights}")

df_data = {'User_ID': [], 'Sorted_Friend_ID': [], 'Sorted_Weights': []}

for user_id, sorted_weights in sorted_combined_weights.items():
    for friend_id, weight in sorted_weights:
        df_data['User_ID'].append(user_id)
        df_data['Sorted_Friend_ID'].append(friend_id)
        df_data['Sorted_Weights'].append(weight)

df = pd.DataFrame(df_data)
df.to_csv('sorted_combined_weights.csv', index=False)

top_n_recommendations = {}
top_n = 10
top_pois = 20

for user_id, sorted_weights in sorted_combined_weights.items():
    top_n_recommendations[user_id] = []
    for friend_id, _ in sorted_weights[:top_n]:
        friend_pois = checkins_data[checkins_data['uid1'] == friend_id]['poi-id'].value_counts().head(top_pois).index.tolist()
        top_n_recommendations[user_id].extend(friend_pois)

for user_id, recommendations in top_n_recommendations.items():
    print(f"User ID: {user_id}, Top {top_pois} Recommendations: {recommendations[:top_pois]}")

df_data = {'User_ID': [], 'Top_N_Recommendations': []}

for user_id, recommendations in top_n_recommendations.items():
    df_data['User_ID'].append(user_id)
    df_data['Top_N_Recommendations'].append(recommendations[:top_pois])

df = pd.DataFrame(df_data)
df.to_csv('explicit_top10.csv', index=False)

"""**RSTSE-V**"""

checkins_data=pd.read_csv('train_timestamp_gowalla.csv')
friends_data = pd.read_csv('Gowalla_social_relations.txt', sep='\t', header=None, names = ['uid1', 'uid2'])

from scipy.spatial.distance import jaccard
checkins_data['timestamp'] = pd.to_datetime(checkins_data['timestamp'])

def calculate_preferences_distribution(user_checkins):
    user_distribution = user_checkins['poi-id'].value_counts(normalize=True).sort_index()
    return user_distribution

def calculate_jaccard_similarity(user1_distribution, user2_distribution):
    intersection = len(set(user1_distribution.index) & set(user2_distribution.index))
    union = len(set(user1_distribution.index) | set(user2_distribution.index))
    return intersection / union if union != 0 else 0

co_occurrences = {}
time_decayed_similarities = {}

for user_id, user_checkins in checkins_data.groupby('uid1'):
    user_distribution = calculate_preferences_distribution(user_checkins)
    co_occurrences[user_id] = {}
    time_decayed_similarities[user_id] = {}

    for u2_user_id, u2_checkins in checkins_data.groupby('uid1'):
        if user_id != u2_user_id:
            u2_distribution = calculate_preferences_distribution(u2_checkins)

            common_pois = user_distribution.index.intersection(u2_distribution.index)
            if not common_pois.empty:
                co_occurrences[user_id][u2_user_id] = common_pois
                user1_recent_timestamp = user_checkins['timestamp'].max()
                user2_recent_timestamp = u2_checkins['timestamp'].max()
                time_difference = np.abs(user1_recent_timestamp - user2_recent_timestamp).total_seconds()
                time_decay = np.exp(-0.1 * time_difference)
                jaccard_similarity = calculate_jaccard_similarity(user_distribution, u2_distribution)
                time_decayed_similarities[user_id][u2_user_id] = jaccard_similarity * time_decay

print("Co-Occurrences:", co_occurrences)
print("Time-Decayed Similarities:", time_decayed_similarities)

sorted_similar_users = {user_id: sorted(similarities.items(), key=lambda x: x[1], reverse=True) for user_id, similarities in time_decayed_similarities.items()}

print(sorted_similar_users)

df_data = {'User_ID': [], 'Similar_User': [], 'Time_Decayed_Similarity': []}
for user_id, similarities in sorted_similar_users.items():
  for similar_user, similarity in similarities:
    df_data['User_ID'].append(user_id)
    df_data['Similar_User'].append(similar_user)
    df_data['Time_Decayed_Similarity'].append(time_decayed_similarities[user_id][similar_user])

df = pd.DataFrame(df_data)
df.to_csv('RSTSE_V_similar_user.csv', index=False)


checkins_data= pd.DataFrame(checkins_data)
grouped_df4 = checkins_data.groupby('uid1').agg({'poi-id': lambda x: list(x), 'freq': lambda x: list(x)}).reset_index()
grouped_df4.columns = ['uid1', 'grouped_poi_ids', 'Grouped_Frequencies']

def recommend_pois(row):
    similar_users = row['similar_users']
    similarity_measure = row['time_decayed_similarity']
    visited_pois_user = set(row['grouped_poi_ids'])
    freq_user = row['Grouped_Frequencies']

    recommended_pois = []
    similar_users_sorted = sorted(zip(similar_users, similarity_measure), key=lambda x: x[1], reverse=True)
    sorted_visited_pois = sorted(zip(visited_pois_user, freq_user), key=lambda x: x[1], reverse=True)
    top_visited_pois = [poi for poi, _ in sorted_visited_pois[:10]]
    recommended_pois.extend(top_visited_pois)

    for sim_user, _ in similar_users_sorted[:10]:
        similar_user_data = grouped_df4[grouped_df4['uid1'] == sim_user]
        if not similar_user_data.empty:
            visited_pois_sim_user = similar_user_data['grouped_poi_ids'].iloc[0]
            freq_sim_user = similar_user_data['Grouped_Frequencies'].iloc[0]
            sorted_visited_pois_sim_user = sorted(zip(visited_pois_sim_user, freq_sim_user), key=lambda x: x[1], reverse=True)
            top_pois_sim_user = [poi for poi, _ in sorted_visited_pois_sim_user[:10]]
            recommended_pois.extend(poi for poi in top_pois_sim_user if poi not in recommended_pois)

    return recommended_pois

df_data['Recommended_POIs'] = df_data.apply(recommend_pois, axis=1)
df_data[['uid1', 'similar_users', 'time_decayed_similarity', 'Recommended_POIs']]
output_file= 'spatial_cooccurences_top_10.csv'
df_data[['uid1', 'similar_users', 'time_decayed_similarity', 'grouped_poi_ids', 'Recommended_POIs']].to_csv(output_file, index=False)

"""**RSTSE-Trans**"""

checkins_df = checkins_data
friends_df = friends_data

def calculate_adamic_adar_score(user1, user2, friends_df):
    common_friends = set(friends_df[friends_df['uid1'] == user1]['uid2']).intersection(set(friends_df[friends_df['uid1'] == user2]['uid2']))

    score = 0
    for friend in common_friends:
        degree = len(friends_df[friends_df['uid1'] == friend])
        if degree > 1:
            score += 1 / np.log(degree)

    return score

def find_rare_common_neighbors(user_a, user_b, friends_df):
    friends_a = set(friends_df[friends_df['uid1'] == user_a]['uid2'])
    friends_b = set(friends_df[friends_df['uid1'] == user_b]['uid2'])

    common_neighbors = friends_a.intersection(friends_b)

    rare_common_neighbors = [neighbor for neighbor in common_neighbors if len(friends_df[friends_df['uid1'] == neighbor]) <= 2]

    return rare_common_neighbors

def check_recency_with_neighbor(user, neighbor, checkins_df, time_threshold=7):
    common_checkins = set(checkins_df[checkins_df['uid1'] == user]['poi-id']).intersection(set(checkins_df[checkins_df['uid1'] == neighbor]['poi-id']))

    for poi in common_checkins:
        user_last_visit = checkins_df[(checkins_df['uid1'] == user) & (checkins_df['poi-id'] == poi)]['timestamp'].max()
        neighbor_last_visit = checkins_df[(checkins_df['uid1'] == neighbor) & (checkins_df['poi-id'] == poi)]['timestamp'].max()

        if user_last_visit and neighbor_last_visit:
            time_diff = np.abs(pd.to_datetime(user_last_visit) - pd.to_datetime(neighbor_last_visit)).total_seconds() / (3600.0 * 24.0)
            if time_diff <= time_threshold:
                return True

    return False

for user_a in friends_df['uid1'].unique():
    user_a_friends = friends_df[friends_df['uid1'] == user_a]['uid2']
    for user_c in user_a_friends:
        user_b_candidates = friends_df[(friends_df['uid1'] == user_c) & ~(friends_df['uid2'].isin(user_a_friends))]['uid2']
        for user_b in user_b_candidates:
            adamic_adar_score = calculate_adamic_adar_score(user_a, user_b, friends_df)
            rare_common_neighbors = find_rare_common_neighbors(user_a, user_b, friends_df)
            recency_weighted_score = 0
            for neighbor in rare_common_neighbors:
                recency_weighted_score += adamic_adar_score * check_recency_with_neighbor(user_a, neighbor, checkins_df)

            print(f'Recency-Weighted Adamic-Adar score between {user_a} and {user_b}: {recency_weighted_score}')

recommendations = {}

for user_a in friends_df['uid1'].unique():
    user_a_friends = friends_df[friends_df['uid1'] == user_a]['uid2']
    for user_c in user_a_friends:
        user_b_candidates = friends_df[(friends_df['uid1'] == user_c) & ~(friends_df['uid2'].isin(user_a_friends))]['uid2']
        for user_b in user_b_candidates:
            adamic_adar_score = calculate_adamic_adar_score(user_a, user_b, friends_df)
            rare_common_neighbors = find_rare_common_neighbors(user_a, user_b, friends_df)
            recency_weighted_score = 0
            for neighbor in rare_common_neighbors:
                recency_weighted_score += adamic_adar_score * check_recency_with_neighbor(user_a, neighbor, checkins_df)
            if recency_weighted_score != 0:
                user_a_visited_pois = set(checkins_df[checkins_df['uid1'] == user_a]['poi-id'])
                user_b_visited_pois = set(checkins_df[checkins_df['uid1'] == user_b]['poi-id'])
                recommended_pois = user_b_visited_pois - user_a_visited_pois
                recommendations[(user_a, user_b)] = recommended_pois
                print(f'Recommendations for {user_a} from {user_b}: {recommended_pois}')

with open('trans.txt', 'w') as file:
    for pair, pois in recommendations.items():
        file.write(f"Recommendations for {pair[0]} from {pair[1]}: {pois}\n")

recommendations_file = 'trans.txt'
user_a_list = []
user_b_list = []
recommended_pois_list = []

with open(recommendations_file, 'r') as file:
    for line in file:
        parts = line.split(':')
        users_part = parts[0].split('from')
        user_a = users_part[0].strip().split()[-1]
        user_b = users_part[1].strip().split()[-1]
        recommended_pois = parts[1].strip()
        user_a_list.append(user_a)
        user_b_list.append(user_b)
        recommended_pois_list.append(recommended_pois)

recommendations_df = pd.DataFrame({'User_A': user_a_list, 'User_B': user_b_list, 'Recommended_POIs': recommended_pois_list})
output_csv_file = 'RSTSE-Trans_Recommended_poi.csv'
recommendations_df.to_csv(output_csv_file, index=False)

"""**RSTSE-T**"""

data=checkins_data
def calculate_penalization(t_theta):
    if t_theta > 6:
        return np.log(1 + 0.8 * t_theta)
    else:
        return np.log(1 + 0.6 * t_theta)

working_hours = (8, 20)
leisure_hours = (20, 8)

data['timestamp'] = pd.to_datetime(data['timestamp'])
data['hour'] = data['timestamp'].dt.hour
data['weekday'] = data['timestamp'].dt.weekday

data['time_difference'] = (data['timestamp'].dt.hour % 24 - working_hours[0]) % 24
data['penalization'] = data['time_difference'].apply(calculate_penalization)

user_poi_matrix = pd.pivot_table(data, values='penalization', index='uid1', columns='poi-id', aggfunc='sum', fill_value=0)

user_poi_matrix = user_poi_matrix.fillna(0)

similarities = []
for user1 in user_poi_matrix.index:
    for user2 in user_poi_matrix.index:
        if user1 != user2:
            numerator = np.sum(user_poi_matrix.loc[user1] * user_poi_matrix.loc[user2])
            denominator = np.sqrt(np.sum(user_poi_matrix.loc[user1]**2) * np.sum(user_poi_matrix.loc[user2]**2))
            similarity = numerator / denominator
            similarities.append((user1, user2, similarity))

similarities.sort(key=lambda x: (x[0], x[2]), reverse=True)

output_file = 'user_similarities.csv'
header = ['User1', 'User2', 'Similarity']

df_similarities = pd.DataFrame(similarities, columns=header)
df_similarities = df_similarities.groupby('User1').agg({'User2': list, 'Similarity': list}).reset_index()
df_similarities.to_csv(output_file, index=False)
a = df_similarities.drop(columns=['Similarity'])
a.to_csv('similar_user.csv', index=False)

merged_df = pd.merge(df_similarities, data, left_on='User1', right_on='uid1', how='inner')
user_poi_dict = {}
for index, row in merged_df.iterrows():
    user_id = row['User1']
    poi = row['poi-id']

    if user_id not in user_poi_dict:
        user_poi_dict[user_id] = set()

    user_poi_dict[user_id].add(poi)

recommended_pois_dict = {}

for index, row in merged_df.iterrows():
    user_id = row['User1']
    similar_users = row['User2']

    if user_id not in recommended_pois_dict:
        recommended_pois_dict[user_id] = set()

    recommended_pois_dict[user_id].update(set(user_poi_dict[user_id]))

    for similar_user in similar_users:
        recommended_pois_dict[user_id].difference_update(user_poi_dict.get(similar_user, set()))

for user_id, recommended_pois in recommended_pois_dict.items():
    print(f"User ID: {user_id}, Recommended POIs: {recommended_pois}")

users = []
similar_users_list = []
recommended_pois_list = []

# Iterate over recommended_pois_dict to collect data
for user_id, recommended_pois in recommended_pois_dict.items():
    similar_users = df_similarities[df_similarities['User1'] == user_id]['User2'].iloc[0]
    similar_users_list.append(similar_users)
    recommended_pois_list.append(list(recommended_pois))
    users.append(user_id)

new_df = pd.DataFrame({'user_id': users, 'similar_users': similar_users_list, 'recommended_pois': recommended_pois_list})
new_df.to_csv('RSTSE_T_recommended_pois.csv', index=False)

"""**RSTSE-D**"""

poi_coords_data= pd.read_csv('Gowalla_poi_coos.txt', sep='\t', header=None, names = ['poi-id', 'lat','lon'])
df1 = checkins_data
df2 = poi_coords_data

df1['user_lat'] = 30.264627
df1['user_lon'] = -97.739482

def haversine(lat1, lon1, lat2, lon2):
    coords_1 = (lat1, lon1)
    coords_2 = (lat2, lon2)
    return geodesic(coords_1, coords_2).km

threshold_distance = 20
recommendations = []

for _, user in df1.iterrows():
    user_lat = user['user_lat']
    user_lon = user['user_lon']
    nearby_pois = df2[df2.apply(lambda x: haversine(user_lat, user_lon, x['lat'], x['lon']), axis=1) <= threshold_distance]
    similar_users = df1[df1['poi-id'].isin(nearby_pois['poi-id'])]
    similar_users = similar_users[similar_users['uid1'] != user['uid1']]
    activity_ratio = similar_users.groupby('uid1').size() / len(similar_users)
    sorted_users = activity_ratio.sort_values(ascending=False)
    recommended_pois = df1[df1['uid1'].isin(sorted_users.index)]['poi-id'].unique()
    recommendations.append((user['uid1'], sorted_users.index.tolist(), recommended_pois.tolist()))
    print(f"Similar users for user {user['uid1']}: {sorted_users.index.tolist()}")

recommendations_df = pd.DataFrame(recommendations, columns=['user-id', 'similar-users', 'recommended-pois'])
recommendations_df.to_csv('RSTSE-D_user_pois.csv', index=False)

"""**Combining pois**"""

file1_path = 'explicit_top10.csv'
file2_path = 'spatial_cooccurences_top_10.csv'
file3_path = 'RSTSE-Trans_Recommended_poi.csv'
file4_path ='RSTSE_T_recommended_pois.csv'
file5_path= 'RSTSE-D_user_pois.csv'

d1 = pd.read_csv(file1_path)
d2 = pd.read_csv(file2_path)
d3 = pd.read_csv(file3_path)
d4 = pd.read_csv(file4_path)
d5 = pd.read_csv(file5_path)
#------------Processing-----------------------------
d4=d4.drop(columns['similar-users'])

d5=d5.drop(columns['similar-users'])

d1 = d1.rename(columns={d1.columns[0]: 'user-id', d1.columns[1]: 'POIs'})
d2 = d2.rename(columns={d2.columns[0]: 'user-id' ,d2.columns[1]: 'POIs'})
d3 = d3.rename(columns={d3.columns[0]: 'user-id', d3.columns[1]: 'POIs'})
d4 = d4.rename(columns={d4.columns[0]: 'user-id', d4.columns[1]: 'POIs'})
d5 = d5.rename(columns={d5.columns[0]: 'user-id', d5.columns[1]: 'POIs'})

result_df = pd.DataFrame(d1)
result_df['recommended_pois'] = result_df['POIs'].apply(ast.literal_eval)
def merge_lists_preserve_order(lists):
    merged_list = []
    seen = set()
    for sublist in lists:
        for item in sublist:
            if item not in seen:
                merged_list.append(item)
                seen.add(item)
    return merged_list

combined_result_df = result_df.groupby('user-id')['recommended_pois'].agg(merge_lists_preserve_order).reset_index()
combined_result_df.columns = ['user-id', 'merged_pois']
print(combined_result_df)

#----------------------------d2--------------------------------
result_df = pd.DataFrame(d2)
result_df['recommended_pois'] = result_df['POIs'].apply(ast.literal_eval)
def merge_lists_preserve_order(lists):
    merged_list = []
    seen = set()
    for sublist in lists:
        for item in sublist:
            if item not in seen:
                merged_list.append(item)
                seen.add(item)
    return merged_list

combined_result_df2 = result_df.groupby('user-id')['recommended_pois'].agg(merge_lists_preserve_order).reset_index()
combined_result_df2.columns = ['user-id', 'merged_pois']
print(combined_result_df2)

#--------------------d3----------------------------------
result_df = pd.DataFrame(d3)
result_df['recommended_pois'] = result_df['POIs'].apply(ast.literal_eval)
def merge_lists_preserve_order(lists):
    merged_list = []
    seen = set()
    for sublist in lists:
        for item in sublist:
            if item not in seen:
                merged_list.append(item)
                seen.add(item)
    return merged_list

combined_result_df3 = result_df.groupby('user-id')['recommended_pois'].agg(merge_lists_preserve_order).reset_index()
combined_result_df3.columns = ['user-id', 'merged_pois']
print(combined_result_df3)

#---------------------------------d4------------------------------
result_df = pd.DataFrame(d4)
result_df['recommended_pois'] = result_df['POIs'].apply(ast.literal_eval)

def merge_lists_preserve_order(lists):
    merged_list = []
    seen = set()
    for sublist in lists:
        for item in sublist:
            if item not in seen:
                merged_list.append(item)
                seen.add(item)
    return merged_list

combined_result_df4 = result_df.groupby('user-id')['recommended_pois'].agg(merge_lists_preserve_order).reset_index()

combined_result_df4.columns = ['user-id', 'merged_pois']
print(combined_result_df4)

#-----------------------d5---------------------------------
result_df['recommended_pois'] = d5['POIs'].apply(ast.literal_eval)

def merge_lists_preserve_order(lists):
    merged_list = []
    seen = set()
    for sublist in lists:
        if isinstance(sublist, list):
            for item in sublist:
                if item not in seen:
                    merged_list.append(item)
                    seen.add(item)
    return merged_list

combined_result_df5 = result_df.groupby('user-id')['recommended_pois'].agg(merge_lists_preserve_order).reset_index()
combined_result_df5.columns = ['user-id', 'merged_pois']
print(combined_result_df5)
#-----------------Alloting--------------------------
df1= combined_result_df
df2= combined_result_df2
df3= combined_result_df3
df4= combined_result_df4
df5= combined_result_df5

l1 = 0.13
alpha = 0.3
beta =0.28
gamma =0.22
delta =0.2

# Merge the dataframes based on the 'user-id' column
merged_df = pd.DataFrame(columns=['user-id', 'contributed_records'])

# Merge df1 and control the number of records contributed by df1 based on alpha
if not df1.empty:
    df1['contributed_records'] = df1['combined_recommended_pois'].apply(lambda x: x[:int(len(x)*l1)])
    merged_df = pd.concat([merged_df, df1[['user-id', 'contributed_records']]], ignore_index=True)

# # Merge df2 and control the number of records contributed by df2 based on beta
if not df2.empty:
  df2['contributed_records'] = df2['combined_recommended_pois'].apply(lambda x: x[:int(len(x)*alpha)])
  merged_df = pd.concat([merged_df, df2[['user-id', 'contributed_records']]], ignore_index=True)

# Merge df3 and control the number of records contributed by df3 based on gamma
if not df3.empty:
    df3['contributed_records'] = df3['combined_recommended_pois'].apply(lambda x: x[:int(len(x)*beta)])
    merged_df = pd.concat([merged_df, df3[['user-id', 'contributed_records']]], ignore_index=True)

# Merge df4 and control the number of records contributed by df4 based on delta
if not df4.empty:
    df4['contributed_records'] = df4['combined_recommended_pois'].apply(lambda x: x[:int(len(x)*gamma)])
    merged_df = pd.concat([merged_df, df4[['user-id', 'contributed_records']]], ignore_index=True)

# #Merge df5 without controlling the number of records contributed
if not df5.empty:
    df5['contributed_records'] = df5['combined_recommended_pois'].apply(lambda x: x[:int(len(x)*delta)])
    merged_df = pd.concat([merged_df, df5[['user-id', 'contributed_records']]], ignore_index=True)

merged_df = merged_df.groupby('user-id')['contributed_records'].apply(sum).reset_index()
print(merged_df)

output_file = 'merged_POIs.csv'
merged_df.to_csv(output_file, index=False)

"""**Friends**"""

file1_path = 'sorted_combined_weights.csv'
file2_path = 'RSTSE_V_similar_user.csv'
file3_path = 'RSTSE-Trans_Recommended_poi.csv'
file4_path = 'RSTSE_T_recommended_pois.csv'
file5_path = 'RSTSE-D_user_pois.csv'

df1 = pd.read_csv(file1_path)
df2 = pd.read_csv(file2_path)
df3 = pd.read_csv(file3_path)
df4 = pd.read_csv(file4_path)
df5 = pd.read_csv(file5_path)

#----------------------------------Processing----------------------------
df1=df1.drop(columns=['Sorted_Weights'])
df2=df2.drop(columns=['Recommended_POIs'])
df3=df3.drop(coluns=['Recommended_POIs'])
df4=df4.drop(columns=['recommended_pois'])
df5=df5.drop(columns=['recommended-pois'])

df5['user-id'] = df5['user-id'].astype(int)
df5['user-id'].dtype

df1 = df1.rename(columns={df1.columns[0]: 'user-id', df1.columns[1]: 'similar-users_ab1'})
df2 = df2.rename(columns={df2.columns[0]: 'user-id' ,df2.columns[1]: 'similar-users_ab2'})
df3 = df3.rename(columns={df3.columns[0]: 'user-id', df3.columns[1]: 'similar-users_ab3'})
df4 =df4.rename(columns={df4.columns[0]: 'user-id', df4.columns[1]: 'similar-users_ab4'})
df5 = df5.rename(columns={df5.columns[0]: 'user-id', df5.columns[1]: 'similar-users_ab5'})

#-------------------------------------
df = pd.DataFrame(df1)
grouped_df = df.groupby('user-id')['similar-users_ab1'].agg(list).reset_index()
result_df = grouped_df.groupby('user-id')['similar-users_ab1'].apply(lambda x: ','.join(map(str, x))).reset_index()
result_df.columns = ['userid', 'merged_similar_users']
print(result_df)
#--------------------------------------
df = pd.DataFrame(df2)
grouped_df = df.groupby('user-id')['similar-users_ab2'].agg(list).reset_index()
result_df2 = grouped_df.groupby('user-id')['similar-users_ab2'].apply(lambda x: ','.join(map(str, x))).reset_index()
result_df2.columns = ['userid', 'merged_similar_users']
print(result_df2)
#--------------------------------------
df = pd.DataFrame(df3)
grouped_df = df.groupby('user-id')['similar-users_ab3'].agg(list).reset_index()
result_df3 = grouped_df.groupby('user-id')['similar-users_ab3'].apply(lambda x: ','.join(map(str, x))).reset_index()
result_df3.columns = ['userid', 'merged_similar_users']
print(result_df3)
#----------------------------------------
df = pd.DataFrame(df4)
grouped_df = df.groupby('user-id')['similar-users_ab4'].agg(list).reset_index()
result_df4 = grouped_df.groupby('user-id')['similar-users_ab4'].apply(lambda x: ','.join(map(str, x))).reset_index()
result_df4.columns = ['userid', 'merged_similar_users']
print(result_df4)
#----------------------------------------
df = pd.DataFrame(df5)
grouped_df = df.groupby('user-id')['similar-users_ab5'].agg(list).reset_index()
result_df5 = grouped_df.groupby('user-id')['similar-users_ab5'].apply(lambda x: ','.join(map(str, x))).reset_index()
result_df5.columns = ['userid', 'merged_similar_users']
print(result_df5)
#------------------------------------------------------------------------
merged_result = pd.merge(result_df, result_df2, on='userid', how='outer')
merged_result = pd.merge(merged_result, result_df3, on='userid', how='outer')
merged_result = pd.merge(merged_result, result_df4, on='userid', how='outer')
merged_result = pd.merge(merged_result, result_df5, on='userid', how='outer')
merged_result['combined_merged_similar_users'] = merged_result[['merged_similar_users_x', 'merged_similar_users_y', 'merged_similar_users_x', 'merged_similar_users_y', 'merged_similar_users_x']].fillna('').agg(lambda x: ','.join(filter(None, x)), axis=1)

merged_result = merged_result[['userid', 'combined_merged_similar_users']]
print(merged_result)
output_file = 'merged_similar_users.csv'
merged_result.to_csv(output_file, index=False)

"""**padding embeddings**"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import OneHotEncoder

user_poi_file = "merged_POIs.csv"
user_poi_df = pd.read_csv(user_poi_file)
user_poi_df = user_poi_df.rename(columns={user_poi_df.columns[0]: 'userid', user_poi_df.columns[1]: 'merged_pois'})

user_similar_users_file = "merged_similar_users.csv"
user_similar_users_df = pd.read_csv(user_similar_users_file)
user_similar_users_df = user_similar_users_df.rename(columns={user_similar_users_df.columns[0]: 'userid', user_similar_users_df.columns[1]: 'similar_users'})

def create_user_embeddings(data):
    vectorizer = CountVectorizer()
    onehot_encoder = OneHotEncoder(sparse=False)
    user_ids = data['userid'].astype(str).tolist()
    user_ids_onehot = onehot_encoder.fit_transform(np.array(user_ids).reshape(-1, 1))
    return user_ids_onehot

def create_poi_embeddings(data):
    poi_values = [set(map(str, value)) for value in data['merged_pois']]
    unique_pois = list(set().union(*poi_values))
    poi_index_mapping = {poi: idx for idx, poi in enumerate(unique_pois)}

    poi_values_binary = []
    for value in poi_values:
        binary_encoding = [1 if poi in value else 0 for poi in unique_pois]
        poi_values_binary.append(binary_encoding)
    poi_values_binary = np.array(poi_values_binary)
    return poi_values_binary

def create_similar_users_embeddings(data, all_user_ids):
    similar_users_embedding = np.zeros((len(all_user_ids), len(all_user_ids)))

    for idx, user_id in enumerate(all_user_ids):
        try:
            similar_users_str = data.loc[data['userid'] == user_id, 'similar_users'].iloc[0]
        except IndexError:
            continue

        if all(user.isdigit() for user in similar_users_str.split(',')):
            similar_users = list(map(int, similar_users_str.split(',')))

            similar_users = [uid for uid in similar_users if uid in all_user_ids]

            similar_users_embedding[idx, [all_user_ids.index(uid) for uid in similar_users]] = 1
    return similar_users_embedding

user_embeddings = create_user_embeddings(user_poi_df)

poi_embeddings = create_poi_embeddings(user_poi_df)

all_user_ids = [int(user_id) if str(user_id).isdigit() else user_id for user_id in user_similar_users_df['userid']]

similar_users_embeddings = create_similar_users_embeddings(user_similar_users_df, all_user_ids)

max_dimension = max(user_embeddings.shape[1], poi_embeddings.shape[1], similar_users_embeddings.shape[0])

user_embeddings = np.pad(user_embeddings, ((0, 0), (0, max_dimension - user_embeddings.shape[1])), 'constant')

poi_values_binary = np.pad(poi_embeddings, ((0, 0), (0, max_dimension - poi_embeddings.shape[1])), 'constant')

similar_users_embeddings = np.pad(similar_users_embeddings, ((0, max_dimension - similar_users_embeddings.shape[0]),
                                                               (0, max_dimension - similar_users_embeddings.shape[1])), 'constant')

print("User Embedding Shape:", user_embeddings.shape)
print("POI Embedding Shape:", poi_embeddings.shape)
print("Similar Users Embedding Shape:", similar_users_embeddings.shape)

from keras import backend as K

def recall(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1(y_true, y_pred):
    precisions = precision(y_true, y_pred)
    recalls = recall(y_true, y_pred)
    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))

import time
from memory_profiler import memory_usage
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, Attention
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

start_time = time.time()

mem_usage_before = memory_usage()[0]

user_poi_interaction_df = pd.merge(user_poi_df, user_similar_users_df, on='userid', how='inner')

train_df= user_poi_interaction_df

scaler = StandardScaler()
user_poi_embedding_scaled = scaler.fit_transform(user_embeddings)
poi_embeddings_scaled = scaler.fit_transform(poi_embeddings)
similar_users_embeddings_scaled = scaler.fit_transform(similar_users_embeddings)

user_encoder = LabelEncoder()
poi_encoder = LabelEncoder()

user_ids_encoded = user_encoder.fit_transform(user_poi_interaction_df['userid'])

poi_ids_encoded = poi_encoder.fit_transform(user_poi_interaction_df['merged_pois'])

num_classes = len(np.unique(poi_ids_encoded))
l_train_df=to_categorical(train['freq'], num_classes=num_classes)


def create_ncf_model_with_attention(num_users, num_pois, embedding_size=64, hidden_units=(32, 16), dropout_rate=0.6, l2_reg_user=0.6, l2_reg_poi=0.6):
    user_input = Input(shape=(1,), name='user_input')

    poi_input = Input(shape=(1,), name='poi_input')

    similar_users_input = Input(shape=(similar_users_embeddings_scaled.shape[1],), name='similar_users_input')

    gmf_user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size, embeddings_regularizer=l2(0.8))(user_input)
    gmf_user_embedding = Flatten()(gmf_user_embedding)

    gmf_poi_embedding = Embedding(input_dim=num_pois, output_dim=embedding_size, embeddings_regularizer=l2(0.6))(poi_input)
    gmf_poi_embedding = Flatten()(gmf_poi_embedding)

    gmf_output = Concatenate()([gmf_user_embedding, gmf_poi_embedding])

    attention_output = Attention()([gmf_output, gmf_output])

    mlp_user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size, embeddings_regularizer=l2(0.3))(user_input)
    mlp_user_embedding = Flatten()(mlp_user_embedding)

    mlp_poi_embedding = Embedding(input_dim=num_pois, output_dim=embedding_size, embeddings_regularizer=l2(0.3))(poi_input)
    mlp_poi_embedding = Flatten()(mlp_poi_embedding)

    mlp_output = Concatenate()([mlp_user_embedding, mlp_poi_embedding, similar_users_input])

    user_poi_concat = Concatenate()([attention_output, mlp_output])
    user_poi_concat = Dropout(0.5)(user_poi_concat)

    for units in hidden_units:
        user_poi_concat = Dense(units, activation='relu', kernel_regularizer=l2(0.6))(user_poi_concat)
        user_poi_concat = Dropout(dropout_rate)(user_poi_concat)

    output_layer = Dense(num_classes, activation='sigmoid')(user_poi_concat)
    model = Model(inputs=[user_input, poi_input, similar_users_input], outputs=output_layer)

    return model

num_users = len(user_encoder.classes_)
num_pois = len(poi_encoder.classes_)

ncf_model_with_attention = create_ncf_model_with_attention(num_users, num_pois)

ncf_model_with_attention.compile(optimizer=Adam(learning_rate=0.00001), loss= MeanSquaredError() , metrics=['accuracy', f1, recall, precision])
ncf_model_with_attention.summary()

train_inputs = [user_ids_encoded[train_df.index], poi_ids_encoded[train_df.index], similar_users_embeddings_scaled[train_df.index]]
train_outputs = l_train_df
train_outputs=train_outputs[:len(train_inputs[0])]

# #---------------------Validation---------------------------------------------------------
val=pd.read_csv('Gowalla_tune.txt',sep='\t', names=['userid','poi-id','freq'])
val=pd.DataFrame(val)
val=val[0:user_ids_encoded.shape[0]]
l_val_df= to_categorical(val['freq'], num_classes=num_classes)
val_inputs = [user_ids_encoded[val.index], poi_ids_encoded[val.index], similar_users_embeddings_scaled[val.index]]
#-----------------------------------------------------------------------------------------
early_stopping = EarlyStopping(monitor='accuracy', patience=3, restore_best_weights=True)
history = ncf_model_with_attention.fit(train_inputs, train_outputs, epochs=50, batch_size=256, callbacks=[early_stopping], validation_data= [val_inputs, l_val_df])
end_time = time.time()
mem_usage_after = memory_usage()[0]

time_taken = end_time - start_time

mem_used = mem_usage_after - mem_usage_before

print(f"Time taken: {time_taken} seconds")
print(f"Memory used: {mem_used} MB")

# ----------------------------------------------------Testing--------------------------------
test_df= pd.read_csv('Gowalla_test.txt', sep='\t', names=['userid','poi-id','freq'])
test_df=test_df[0:user_ids_encoded.shape[0]]
test_inputs = [user_ids_encoded[test_df.index], poi_ids_encoded[test_df.index], similar_users_embeddings_scaled[test_df.index]]
l_val_df= to_categorical(test_df['freq'], num_classes=num_classes)

loss, accuracy, f1, recall, precision = ncf_model_with_attention.evaluate(test_inputs, l_val_df)
print(f'Test Loss: {loss}, Test Accuracy: {accuracy}, Test f1: {f1}, Test precision: {precision}, Test recall: {recall}')

predictions = ncf_model_with_attention.predict(test_inputs)

from sklearn.metrics import ndcg_score
true_relevance = []
predicted_relevance = []

predictions = ncf_model_with_attention.predict(test_inputs)

for i, (user_id, poi_id, freq) in enumerate(test_df[['userid', 'poi-id', 'freq']].values):
    predicted_poi_score = predictions[i][0]

    true_relevance.append(freq)
    predicted_relevance.append(predicted_poi_score)

ndcg = ndcg_score([true_relevance], [predicted_relevance])

print("NDCG Score:", ndcg)

y_true = l_val_df

def compute_recall_at_k(y_true, y_pred, k):
    num_users = len(y_true)
    recall_at_k_sum = 0.0

    for i in range(num_users):
        true_interactions = y_true[i]
        predicted_probs = y_pred[i]

        top_k_indices = np.argsort(predicted_probs)[::-1][:k]

        true_positives = sum(true_interactions[j] for j in top_k_indices)

        total_positives = sum(true_interactions)

        recall_at_k = true_positives / total_positives if total_positives > 0 else 0

        recall_at_k_sum += recall_at_k

    recall_at_k = recall_at_k_sum / num_users

    return recall_at_k


def compute_precision_at_k(y_true, y_pred, k):
    num_users = y_true.shape[0]
    precision_at_k_sum = 0.0

    for i in range(num_users):
        true_interactions = y_true[i]
        predicted_probs = y_pred[i]

        top_k_indices = np.argsort(predicted_probs)[::-1][:k]

        true_positives = np.sum(true_interactions[top_k_indices])

        precision_at_k = true_positives / k

        precision_at_k_sum += precision_at_k
    precision_at_k = precision_at_k_sum / num_users

    return precision_at_k

def compute_ndcg_at_k(y_true, y_pred, k):
    num_users = y_true.shape[0]
    ndcg_at_k_sum = 0.0
    for i in range(num_users):
        true_interactions = y_true[i]
        predicted_scores = y_pred[i]
        top_k_indices = np.argsort(predicted_scores)[::-1][:k]
        relevance_scores = true_interactions[top_k_indices]
        dcg = np.sum(relevance_scores / np.log2(np.arange(2, k + 2)))
        ideal_relevance_scores = np.sort(true_interactions)[::-1][:k]
        ideal_dcg = np.sum(ideal_relevance_scores / np.log2(np.arange(2, k + 2)))

        if ideal_dcg == 0:
            ndcg_at_k = 0
        else:
            ndcg_at_k = dcg / ideal_dcg

        ndcg_at_k_sum += ndcg_at_k
    ndcg_at_k = ndcg_at_k_sum / num_users
    return ndcg_at_k

def compute_f1_score(precision, recall):
    if precision + recall == 0:
        return 0
    f1_score = 2 * (precision * recall) / (precision + recall)
    return f1_score


recall_at_5 = compute_recall_at_k(y_true, predictions, 5)
recall_at_10 = compute_recall_at_k(y_true, predictions, 10)
recall_at_15 = compute_recall_at_k(y_true, predictions, 15)
recall_at_20 = compute_recall_at_k(y_true, predictions, 20)
recall_at_25 = compute_recall_at_k(y_true, predictions, 25)
recall_at_30 = compute_recall_at_k(y_true, predictions, 30)
recall_at_35 = compute_recall_at_k(y_true, predictions, 35)
recall_at_40 = compute_recall_at_k(y_true, predictions, 40)
recall_at_45 = compute_recall_at_k(y_true, predictions, 45)

precision_at_5 = compute_precision_at_k(y_true, predictions, 5)
precision_at_10 = compute_precision_at_k(y_true, predictions, 10)
precision_at_15 = compute_precision_at_k(y_true, predictions, 15)
precision_at_20 = compute_precision_at_k(y_true, predictions, 20)
precision_at_25 = compute_precision_at_k(y_true, predictions, 25)
precision_at_30 = compute_precision_at_k(y_true, predictions, 30)
precision_at_35 = compute_precision_at_k(y_true, predictions, 35)
precision_at_40 = compute_precision_at_k(y_true, predictions, 40)

ndcg_at_5 = compute_ndcg_at_k(y_true, predictions, 5)
ndcg_at_10 = compute_ndcg_at_k(y_true, predictions, 10)
ndcg_at_15 = compute_ndcg_at_k(y_true, predictions, 15)
ndcg_at_20 = compute_ndcg_at_k(y_true, predictions, 20)
ndcg_at_25 = compute_ndcg_at_k(y_true, predictions, 25)
ndcg_at_30 = compute_ndcg_at_k(y_true, predictions, 30)
ndcg_at_35 = compute_ndcg_at_k(y_true, predictions, 35)
ndcg_at_40 = compute_ndcg_at_k(y_true, predictions, 40)

f1_at_5 = compute_f1_score(precision_at_5, recall_at_5)
f1_at_10 = compute_f1_score(precision_at_10, recall_at_10)
f1_at_15 = compute_f1_score(precision_at_15, recall_at_15)
f1_at_20= compute_f1_score(precision_at_20, recall_at_20)


print("Recall@5:", recall_at_5)
print("Recall@10:", recall_at_10)
print("Recall@15:", recall_at_15)
print("Recall@20:", recall_at_20)
print("Recall@25:", recall_at_25)
print("Recall@30:", recall_at_30)
print("Recall@35:", recall_at_35)
print("Recall@40:", recall_at_40)
print("Recall@45:", recall_at_45)

print("Precision@5:", precision_at_5)
print("Precision@10:", precision_at_10)
print("Precision@15:", precision_at_15)
print("Precision@20:", precision_at_20)
print("Precision@25:", precision_at_25)
print("Precision@30:", precision_at_30)
print("Precision@35:", precision_at_35)
print("Precision@40:", precision_at_40)

print("NDCG@5:", ndcg_at_5)
print("NDCG@10:", ndcg_at_10)
print("NDCG@15:", ndcg_at_15)
print("NDCG@20:", ndcg_at_20)
print("NDCG@25:", ndcg_at_25)
print("NDCG@30:", ndcg_at_30)
print("NDCG@35:", ndcg_at_35)
print("NDCG@40:", ndcg_at_40)

print("F1 score@5:", f1_at_5)
print("F1 score@10:", f1_at_10)
print("F1 score@15:", f1_at_15)
print("F1 score@20:", f1_at_20)