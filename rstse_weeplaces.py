# -*- coding: utf-8 -*-
"""RSTSE-Weeplaces.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y_PH7zmjB_yCQuvfvZRRXFCPndPru-HS
"""

import pandas as pd
import numpy as np
from scipy.spatial.distance import jaccard
from geopy.distance import geodesic
from collections import OrderedDict
import ast

checkins_data=pd.read_csv('train_data.csv')
checkins_data = checkins_data.rename(columns={'user-id': 'uid1', 'poi_id':'poi-id','datetime':'timestamp'})
friends_data = pd.read_csv('cleaned_extracted_data.csv')

"""**RSTSE-Implicit**"""

all_locations = checkins_data['poi-id'].unique()

def calculate_kl_divergence(p1, p2, epsilon=1e-10):
    p1 = np.array(p1)
    p2 = np.array(p2)

    p1 = p1 / np.sum(p1)
    p2_sum = np.sum(p2)

    if p2_sum == 0:
        return np.inf

    p2 = p2 / p2_sum

    kl_divergence = np.sum(p1 * np.log((p1 + epsilon) / (p2 + epsilon)))
    return kl_divergence

def calculate_preferences_distribution(user_checkins, all_locations):
    user_distribution = user_checkins.value_counts(normalize=True).sort_index()
    user_distribution = user_distribution.reindex(all_locations, fill_value=0)
    return user_distribution

def calculate_weights_kl_divergence(target_user_preferences, friend_preferences):
    weights_dict = {}
    for friend_id, friend_preferences in friend_preferences.items():
        kl_divergence = calculate_kl_divergence(target_user_preferences, friend_preferences)

        if np.isfinite(kl_divergence):
            weight = 1 / (kl_divergence + 1e-10)
        else:
            weight = 0

        weights_dict[friend_id] = weight

    return weights_dict

checkin_data = checkins_data

friends_data = friends_data

all_locations = checkin_data['poi-id'].unique()

all_weights = {}

for target_user_id in checkin_data['uid1'].unique():
    target_user_checkins = checkin_data[checkin_data['uid1'] == target_user_id]['poi-id']

    target_user_friends_uid1 = friends_data[friends_data['uid1'] == target_user_id]['uid2'].tolist()
    target_user_friends_uid2 = friends_data[friends_data['uid2'] == target_user_id]['uid1'].tolist()
    target_user_friends = target_user_friends_uid1 + target_user_friends_uid2

    friends_checkins = {}
    for friend_id in target_user_friends:
        friend_checkins_data = checkin_data[checkin_data['uid1'] == friend_id]
        friends_checkins[friend_id] = friend_checkins_data['poi-id']

    target_user_preferences = calculate_preferences_distribution(target_user_checkins, all_locations)
    friends_preferences = {friend_id: calculate_preferences_distribution(friends_checkins[friend_id], all_locations) for friend_id in target_user_friends}

    weights_kl_divergence = calculate_weights_kl_divergence(target_user_preferences, friends_preferences)
    all_weights[target_user_id] = weights_kl_divergence

for user_id, weights in all_weights.items():
    print(f"User ID: {user_id}, KL Divergence Weights: {weights}")

def calculate_one_dimensional_power_law_weight(ts, beta):
    if ts.total_seconds() < 0 or beta <= 1:
        return 0
    else:
        return (beta - 1) * (1 + ts.total_seconds()) ** (-beta)
def calculate_recency_weight(target_user_last_checkin_timestamp, friend_last_checkin_timestamp, beta):
    ts_uv = np.abs((target_user_last_checkin_timestamp - friend_last_checkin_timestamp).total_seconds())

    if ts_uv == 0 or beta <= 1:
        return 0
    else:
        recency_weight = 1 - (1 / (1 + ts_uv ** (1 - beta)))
        return recency_weight

def calculate_weights_one_dimensional_power_law(target_user_last_checkin_timestamp, friend_last_checkin_timestamp, beta):
    # Handle missing timestamps
    if pd.isnull(target_user_last_checkin_timestamp) or pd.isnull(friend_last_checkin_timestamp):
        return 0

    ts_uv = np.abs(target_user_last_checkin_timestamp - friend_last_checkin_timestamp)
    recency_weight = calculate_recency_weight(target_user_last_checkin_timestamp, friend_last_checkin_timestamp, beta)
    power_law_weight = calculate_one_dimensional_power_law_weight(ts_uv, beta)

    # Handle NaN values resulting from invalid calculations
    if np.isnan(power_law_weight) or np.isnan(recency_weight):
        return 0
    else:
        return power_law_weight * recency_weight


def calculate_weights_for_all_users(friends_data, checkin_data, beta):
    all_weights = {}

    for target_user_id in friends_data['uid1'].unique():
        target_user_data = checkin_data[checkin_data['uid1'] == target_user_id]
        target_user_last_checkin_timestamp = target_user_data['timestamp'].max()
        friends_user_ids = target_user_data['uid1'].tolist()

        weights = []
        for friend_id in friends_data[friends_data['uid1'] == target_user_id]['uid2']:
            friend_data = checkin_data[checkin_data['uid1'] == friend_id]
            friend_last_checkin_timestamp = friend_data['timestamp'].max()
            weight = calculate_weights_one_dimensional_power_law(target_user_last_checkin_timestamp, friend_last_checkin_timestamp, beta)
            weights.append((friend_id, weight))

        all_weights[target_user_id] = weights

    return all_weights

checkin_data = checkins_data
checkin_data['timestamp'] = pd.to_datetime(checkin_data['timestamp'])


all_locations = checkin_data['poi-id'].unique()

all_weights = {}

for target_user_id in checkin_data['uid1'].unique():
    target_user_checkins = checkin_data[checkin_data['uid1'] == target_user_id]['poi-id']

    target_user_friends_uid1 = friends_data[friends_data['uid1'] == target_user_id]['uid2'].tolist()
    target_user_friends_uid2 = friends_data[friends_data['uid2'] == target_user_id]['uid1'].tolist()
    target_user_friends = target_user_friends_uid1 + target_user_friends_uid2

beta = 1.5

all_weights2 = calculate_weights_for_all_users(friends_data, checkin_data, beta)

for user_id, weights in all_weights2.items():
    print(f"User ID: {user_id}, Weights: {weights}")

combined_weights = {}
alpha = 0.4
for user_id in set(all_weights.keys()).union(all_weights2.keys()):
    weights_kl_divergence = dict(all_weights.get(user_id, []))
    weights_power_law = dict(all_weights2.get(user_id, []))

    friend_ids = set(weights_kl_divergence.keys()).union(weights_power_law.keys())
    combined_weights[user_id] = [(friend_id, alpha* weights_kl_divergence.get(friend_id, 0) + (1-alpha)* weights_power_law.get(friend_id, 0)) for friend_id in friend_ids]

for user_id, weights in combined_weights.items():
    print(f"User ID: {user_id}, Combined Weights: {weights}")

sorted_combined_weights = {}

for user_id, weights in combined_weights.items():
    sorted_weights = sorted(weights, key=lambda x: x[1], reverse=True)
    sorted_combined_weights[user_id] = sorted_weights

for user_id, sorted_weights in sorted_combined_weights.items():
    print(f"User ID: {user_id}, Sorted Combined Weights: {sorted_weights}")

df_data = {'User_ID': [], 'Sorted_Friend_ID': [], 'Sorted_Weights': []}

for user_id, sorted_weights in sorted_combined_weights.items():
    for friend_id, weight in sorted_weights:
        df_data['User_ID'].append(user_id)
        df_data['Sorted_Friend_ID'].append(friend_id)
        df_data['Sorted_Weights'].append(weight)

df = pd.DataFrame(df_data)
df.to_csv('RSTSE_IMPLICT_similar_users.csv', index=False)
top_n_recommendations = {}

top_n = 10
top_pois = 20

for user_id, sorted_weights in sorted_combined_weights.items():
    top_n_recommendations[user_id] = []
    for friend_id, _ in sorted_weights[:top_n]:
        friend_pois = checkins_data[checkins_data['uid1'] == friend_id]['poi-id'].value_counts().head(top_pois).index.tolist()
        top_n_recommendations[user_id].extend(friend_pois)

for user_id, recommendations in top_n_recommendations.items():
    print(f"User ID: {user_id}, Top {top_pois} Recommendations: {recommendations[:top_pois]}")

df_data = {'User_ID': [], 'Top_N_Recommendations': []}

for user_id, recommendations in top_n_recommendations.items():
    df_data['User_ID'].append(user_id)
    df_data['Top_N_Recommendations'].append(recommendations[:top_pois])

df = pd.DataFrame(df_data)
df.to_csv('RSTSE_IMPLICIT_Recommended POIS.csv', index=False)

"""**RSTSE- V**"""

checkins_data['timestamp'] = pd.to_datetime(checkins_data['timestamp'])

def calculate_preferences_distribution(user_checkins):
    user_distribution = user_checkins['poi-id'].value_counts(normalize=True).sort_index()
    return user_distribution

def calculate_jaccard_similarity(user1_distribution, user2_distribution):
    intersection = len(set(user1_distribution.index) & set(user2_distribution.index))
    union = len(set(user1_distribution.index) | set(user2_distribution.index))
    return intersection / union if union != 0 else 0

co_occurrences = {}
time_decayed_similarities = {}

for user_id, user_checkins in checkins_data.groupby('uid1'):
    user_distribution = calculate_preferences_distribution(user_checkins)
    co_occurrences[user_id] = {}
    time_decayed_similarities[user_id] = {}

    for u2_user_id, u2_checkins in checkins_data.groupby('uid1'):
        if user_id != u2_user_id:
            u2_distribution = calculate_preferences_distribution(u2_checkins)

            common_pois = user_distribution.index.intersection(u2_distribution.index)
            if not common_pois.empty:
                co_occurrences[user_id][u2_user_id] = common_pois

                user1_recent_timestamp = user_checkins['timestamp'].max()
                user2_recent_timestamp = u2_checkins['timestamp'].max()
                time_difference = np.abs(user1_recent_timestamp - user2_recent_timestamp).total_seconds()
                time_decay = np.exp(-0.1 * time_difference)
                jaccard_similarity = calculate_jaccard_similarity(user_distribution, u2_distribution)
                time_decayed_similarities[user_id][u2_user_id] = jaccard_similarity * time_decay

print("Co-Occurrences:", co_occurrences)
print("Time-Decayed Similarities:", time_decayed_similarities)

sorted_similar_users = {user_id: sorted(similarities.items(), key=lambda x: x[1], reverse=True) for user_id, similarities in time_decayed_similarities.items()}

print(sorted_similar_users)

df_data = {'User_ID': [], 'Similar_User': [], 'Time_Decayed_Similarity': []}

for user_id, similarities in sorted_similar_users.items():
    for similar_user, similarity in similarities:
        df_data['User_ID'].append(user_id)
        df_data['Similar_User'].append(similar_user)
        df_data['Time_Decayed_Similarity'].append(time_decayed_similarities[user_id][similar_user])

df = pd.DataFrame(df_data)
df = df.groupby('User_ID').agg({
    'Similar_User': 'unique',
    'Time_Decayed_Similarity': 'unique'
}).reset_index()
df = df.rename(columns={'User_ID':'uid1', 'Similar_User': 'similar_users', 'Time_Decayed_Similarity': 'time_decayed_similarity'})

df.to_csv('RSTSE_V_similar.csv', index=False)

checkins_data= pd.DataFrame(checkins_data)
grouped_df4 = checkins_data.groupby('uid1').agg({'poi-id': lambda x: list(x), 'freq': lambda x: list(x)}).reset_index()
grouped_df4.columns = ['uid1', 'grouped_poi_ids', 'Grouped_Frequencies']
merged_df = pd.merge(df, grouped_df4, on='uid1')

def recommend_pois(row):
    similar_users = row['similar_users']
    similarity_measure = row['time_decayed_similarity']
    visited_pois_user = set(row['grouped_poi_ids'])
    freq_user = row['Grouped_Frequencies']

    recommended_pois = []

    similar_users_sorted = sorted(zip(similar_users, similarity_measure), key=lambda x: x[1], reverse=True)

    sorted_visited_pois = sorted(zip(visited_pois_user, freq_user), key=lambda x: x[1], reverse=True)
    top_visited_pois = [poi for poi, _ in sorted_visited_pois[:10]]

    recommended_pois.extend(top_visited_pois)

    for sim_user, _ in similar_users_sorted[:10]:
        similar_user_data = grouped_df4[grouped_df4['uid1'] == sim_user]
        if not similar_user_data.empty:
            visited_pois_sim_user = similar_user_data['grouped_poi_ids'].iloc[0]
            freq_sim_user = similar_user_data['Grouped_Frequencies'].iloc[0]

            sorted_visited_pois_sim_user = sorted(zip(visited_pois_sim_user, freq_sim_user), key=lambda x: x[1], reverse=True)
            top_pois_sim_user = [poi for poi, _ in sorted_visited_pois_sim_user[:10]]

            recommended_pois.extend(poi for poi in top_pois_sim_user if poi not in recommended_pois)
    return recommended_pois

merged_df['Recommended_POIs'] = merged_df.apply(recommend_pois, axis=1)

result_df = merged_df[['uid1', 'similar_users', 'time_decayed_similarity', 'grouped_poi_ids', 'Recommended_POIs']]
output_file= 'RSTSE-V_recommended_POIs.csv'
result_df[['uid1', 'similar_users', 'time_decayed_similarity', 'grouped_poi_ids', 'Recommended_POIs']].to_csv(output_file, index=False)

"""**RSTSE-T**"""

data=checkins_data
def calculate_penalization(t_theta):
    if t_theta > 6:
        return np.log(1 + 0.8 * t_theta)
    else:
        return np.log(1 + 0.6 * t_theta)

working_hours = (8, 20)
leisure_hours = (20, 8)

data['timestamp'] = pd.to_datetime(data['timestamp'])
data['hour'] = data['timestamp'].dt.hour
data['weekday'] = data['timestamp'].dt.weekday

data['time_difference'] = (data['timestamp'].dt.hour % 24 - working_hours[0]) % 24
data['penalization'] = data['time_difference'].apply(calculate_penalization)

user_poi_matrix = pd.pivot_table(data, values='penalization', index='uid1', columns='poi-id', aggfunc='sum', fill_value=0)

user_poi_matrix = user_poi_matrix.fillna(0)

similarities = []
for user1 in user_poi_matrix.index:
    for user2 in user_poi_matrix.index:
        if user1 != user2:
            numerator = np.sum(user_poi_matrix.loc[user1] * user_poi_matrix.loc[user2])
            denominator = np.sqrt(np.sum(user_poi_matrix.loc[user1]**2) * np.sum(user_poi_matrix.loc[user2]**2))
            similarity = numerator / denominator
            similarities.append((user1, user2, similarity))

similarities.sort(key=lambda x: (x[0], x[2]), reverse=True)
output_file = 'RSTSE-T_user_similarities.csv'
header = ['User1', 'User2', 'Similarity']
df_similarities = pd.DataFrame(similarities, columns=header)
df_similarities = df_similarities.groupby('User1').agg({'User2': list, 'Similarity': list}).reset_index()
df_similarities = df_similarities.drop(columns=['Similarity'])
df_similarities.to_csv(output_file, index=False)

merged_df = pd.merge(df_similarities, data, left_on='User1', right_on='uid1', how='inner')

user_poi_dict = {}
for index, row in merged_df.iterrows():
    user_id = row['User1']
    poi = row['poi-id']

    if user_id not in user_poi_dict:
        user_poi_dict[user_id] = set()
    user_poi_dict[user_id].add(poi)
recommended_pois_dict = {}

for index, row in merged_df.iterrows():
    user_id = row['User1']
    similar_users = row['User2']

    if user_id not in recommended_pois_dict:
        recommended_pois_dict[user_id] = set()

    recommended_pois_dict[user_id].update(set(user_poi_dict[user_id]))
    for similar_user in similar_users:
        recommended_pois_dict[user_id].difference_update(user_poi_dict.get(similar_user, set()))

for user_id, recommended_pois in recommended_pois_dict.items():
    print(f"User ID: {user_id}, Recommended POIs: {recommended_pois}")

users = []
similar_users_list = []
recommended_pois_list = []

for user_id, recommended_pois in recommended_pois_dict.items():
    similar_users = x[x['User1'] == user_id]['User2'].iloc[0]
    similar_users_list.append(similar_users)
    recommended_pois_list.append(list(recommended_pois))
    users.append(user_id)

new_df = pd.DataFrame({'user_id': users, 'similar_users': similar_users_list, 'recommended_pois': recommended_pois_list})
new_df.to_csv('RSTES-T_recommended_pois.csv', index=False)

"""**RSTSE- Trans**"""

checkins_df = checkins_data

friends_df = friends_data

def calculate_adamic_adar_score(user1, user2, friends_df):
    common_friends = set(friends_df[friends_df['uid1'] == user1]['uid2']).intersection(set(friends_df[friends_df['uid1'] == user2]['uid2']))

    score = 0
    for friend in common_friends:
        degree = len(friends_df[friends_df['uid1'] == friend])
        if degree > 1:
            score += 1 / np.log(degree)

    return score

def find_rare_common_neighbors(user_a, user_b, friends_df):
    friends_a = set(friends_df[friends_df['uid1'] == user_a]['uid2'])
    friends_b = set(friends_df[friends_df['uid1'] == user_b]['uid2'])

    common_neighbors = friends_a.intersection(friends_b)

    rare_common_neighbors = [neighbor for neighbor in common_neighbors if len(friends_df[friends_df['uid1'] == neighbor]) <= 2]

    return rare_common_neighbors

def check_recency_with_neighbor(user, neighbor, checkins_df, time_threshold=7):
    common_checkins = set(checkins_df[checkins_df['uid1'] == user]['poi-id']).intersection(set(checkins_df[checkins_df['uid1'] == neighbor]['poi-id']))

    for poi in common_checkins:
        user_last_visit = checkins_df[(checkins_df['uid1'] == user) & (checkins_df['poi-id'] == poi)]['timestamp'].max()
        neighbor_last_visit = checkins_df[(checkins_df['uid1'] == neighbor) & (checkins_df['poi-id'] == poi)]['timestamp'].max()

        if user_last_visit and neighbor_last_visit:
            time_diff = np.abs(pd.to_datetime(user_last_visit) - pd.to_datetime(neighbor_last_visit)).total_seconds() / (3600.0 * 24.0)
            if time_diff <= time_threshold:
                return True

    return False

for user_a in friends_df['uid1'].unique():
    user_a_friends = friends_df[friends_df['uid1'] == user_a]['uid2']

    for user_c in user_a_friends:
        user_b_candidates = friends_df[(friends_df['uid1'] == user_c) & ~(friends_df['uid2'].isin(user_a_friends))]['uid2']
        for user_b in user_b_candidates:
            adamic_adar_score = calculate_adamic_adar_score(user_a, user_b, friends_df)
            rare_common_neighbors = find_rare_common_neighbors(user_a, user_b, friends_df)
            recency_weighted_score = 0
            for neighbor in rare_common_neighbors:
                recency_weighted_score += adamic_adar_score * check_recency_with_neighbor(user_a, neighbor, checkins_df)

recommendations = {}

for user_a in friends_df['uid1'].unique():
    user_a_friends = friends_df[friends_df['uid1'] == user_a]['uid2']

    for user_c in user_a_friends:
        user_b_candidates = friends_df[(friends_df['uid1'] == user_c) & ~(friends_df['uid2'].isin(user_a_friends))]['uid2']

        for user_b in user_b_candidates:
            adamic_adar_score = calculate_adamic_adar_score(user_a, user_b, friends_df)

            rare_common_neighbors = find_rare_common_neighbors(user_a, user_b, friends_df)

            recency_weighted_score = 0
            for neighbor in rare_common_neighbors:
                recency_weighted_score += adamic_adar_score * check_recency_with_neighbor(user_a, neighbor, checkins_df)

            if recency_weighted_score != 0:
                user_a_visited_pois = set(checkins_df[checkins_df['uid1'] == user_a]['poi-id'])
                user_b_visited_pois = set(checkins_df[checkins_df['uid1'] == user_b]['poi-id'])

                recommended_pois = user_b_visited_pois - user_a_visited_pois

                recommendations[(user_a, user_b)] = recommended_pois

with open('RSTSE-Trans_trans.txt', 'w') as file:
    for pair, pois in recommendations.items():
        file.write(f"Recommendations for {pair[0]} from {pair[1]}: {pois}\n")

recommendations_file = 'RSTSE-Trans_trans.txt'
user_a_list = []
user_b_list = []
recommended_pois_list = []

with open(recommendations_file, 'r') as file:
    for line in file:
        parts = line.split(':')
        users_part = parts[0].split('from')
        user_a = users_part[0].strip().split()[-1]
        user_b = users_part[1].strip().split()[-1]
        recommended_pois = parts[1].strip()
        user_a_list.append(user_a)
        user_b_list.append(user_b)
        recommended_pois_list.append(recommended_pois)

recommendations_df = pd.DataFrame({'User_A': user_a_list, 'User_B': user_b_list, 'Recommended_POIs': recommended_pois_list})
output_csv_file = 'RSTSE-Trans_Recommended_pois.csv'
recommendations_df.to_csv(output_csv_file, index=False)

"""**RSTSE-D**"""

df1 = checkins_data
df1['user_lat'] = 30.264627
df1['user_lon'] = -97.739482

def haversine(lat1, lon1, lat2, lon2):
    coords_1 = (lat1, lon1)
    coords_2 = (lat2, lon2)
    return geodesic(coords_1, coords_2).km

threshold_distance = 100
recommendations = []

for _, user in df1.iterrows():
    user_lat = user['user_lat']
    user_lon = user['user_lon']

    nearby_pois = df1[df1.apply(lambda x: haversine(user_lat, user_lon, x['lat'], x['lon']), axis=1) <= threshold_distance]
    similar_users = df1[df1['poi-id'].isin(nearby_pois['poi-id'])]
    similar_users = similar_users[similar_users['uid1'] != user['uid1']]
    activity_ratio = similar_users.groupby('uid1').size() / len(similar_users)
    sorted_users = activity_ratio.sort_values(ascending=False)
    recommended_pois = df1[df1['uid1'].isin(sorted_users.index)]['poi-id'].unique()
    recommendations.append((user['uid1'], sorted_users.index.tolist(), recommended_pois.tolist()))
    print(f"Similar users for user {user['uid1']}: {sorted_users.index.tolist()}")

recommendations_df = pd.DataFrame(recommendations, columns=['user-id', 'similar-users', 'recommended-pois'])
recommendations_df.to_csv('RSTSE-D_user_pois.csv', index=False)

"""**POIs combining**"""

file1_path = 'RSTSE_IMPLICIT_Recommended POIS.csv'
file2_path = 'RSTSE-V_recommended_POIs.csv'
file3_path ='RSTES-T_recommended_pois.csv'
file4_path = 'RSTSE-Trans_Recommended_pois.csv'
file5_path= 'RSTSE-D_user_pois.csv'

d1 = pd.read_csv(file1_path)
d2 = pd.read_csv(file2_path)
d3 = pd.read_csv(file3_path)
d4 = pd.read_csv(file4_path)
d5 = pd.read_csv(file5_path)
#--------------------Preprocessing----------------------
d2=d2.drop(columns=['similar_users', 'time_decayed_similarity', 'grouped_poi_ids'])
d3=d3.drop(coulmns=['similar_users'])
d4=d4.drop(columns=['User_B'])
d5=d5.drop(columns=['similar-users'])

d1 = d1.rename(columns={d1.columns[0]: 'user-id', d1.columns[1]: 'POIs'})
d2 = d2.rename(columns={d2.columns[0]: 'user-id' ,d2.columns[1]: 'POIs'})
d3 = d3.rename(columns={d3.columns[0]: 'user-id', d3.columns[1]: 'POIs'})
d4 = d4.rename(columns={d4.columns[0]: 'user-id', d4.columns[1]: 'POIs'})
d5 = d5.rename(columns={d5.columns[0]: 'user-id', d5.columns[1]: 'POIs'})

"""**REST OF THE CODE IS SAME AS THE RSTSE.py file**"""